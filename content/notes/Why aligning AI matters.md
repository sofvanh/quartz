---
title: "Why 'aligning' AI matters"
date: "2022-08-14"
tags:
- sapling
- learning
- alignment
- artificial intelligence
---

**[Humans are smart, machines could be even smarter, super-smart machines means big changes, and those changes might not be good.](https://intelligence.org/2015/07/24/four-background-claims/)** Humans have *general intelligence*, which means we are able to solve problems in many different contexts. Once AI (artificial intelligence) actors reach similar intelligence, and beyond, they will be very powerful. There is currently no guarantee that these AGIs (artificial general intelligence) will be *aligned with human values* - in fact [there is evidence to the contrary](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view).

**This is a weird problem, and it is hard to solve.** Most current research is focused either on an engineering-focused approach or a philosophy-focused approach, and [this might not be sufficient](https://bounded-regret.ghost.io/more-is-different-for-ai/). AIs seem to develop capabilities in an [unpredictable manner](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/), and we need [novel ways of thinking](https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/) to progress in this strange field.

**Predicting the progress of developing AGI is also hard.** After all, we are trying to study the development of something that has never been seen before, and we're not sure if we can understand exactly how it will work when it's done. We can use different kinds of "anchors" as helpers when trying to form a view of how the field of AI will progress. [Taking anchors from evolution and biology](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) is a popular schema.

**This is a big thing, AGI will impact everything, and it will probably arrive during this century.** This means we might be living in ["the most important century"](https://www.cold-takes.com/most-important-century/#Summary), the pivotal point in time where the future of humanity is decided.

### Notes on these notes
**This page is a work-in-progress.** I'm writing these notes while reading through the first few weeks of [AGI Safety Fundamentals by EA Cambridge](https://www.eacambridge.org/agi-safety-fundamentals), specifically the technical alignment curriculum. I'm trying to clarify my understanding of the problem and the field by putting all the core readings into their place in a prosaic explanation of the topic.

## Further action
See [[notes/How to help with aligning AI|How to help with aligning AI]]